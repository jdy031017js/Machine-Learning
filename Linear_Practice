import torch  # 텐서 연산 수행
import torch.nn as nn  # 신경망 구성 라이브러리
import torch.optim as optim  # 최적화 알고리즘(옵티마이저)
from torch.utils.data import DataLoader  # 데이터셋을 배치 단위로 불러옴
import torchvision.datasets as datasets  # MNIST 데이터셋 불러오기
import torchvision.transforms as transforms  # 데이터 변환을 위한 라이브러리

# 모델 학습에 GPU가 가능한지 확인 (GPU가 있으면 cuda, 그렇지 않으면 cpu 사용)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class CNN_han(nn.Module):
    def __init__(self):
        super(CNN_han, self).__init__()
        
        # 합성곱 첫 번째층, [64, 1, 28, 28] -> [64, 32, 26, 26]
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)
        
        # 합성곱 두 번째층, [64, 32, 13, 13] -> [64, 64, 11, 11]
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)
        
        # 합성곱 세 번째층,  [64, 64, 5, 5] -> [64, 64, 3, 3]
        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)
        
        # 맥스 풀링층: 커널 크기 2x2, 스트라이드 2
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # 완전 연결층: 64 * 3 * 3 입력, 10 출력
        self.fc = nn.Linear(64 * 3 * 3, 10)

        # ReLU 활성화 함수
        self.relu = nn.ReLU()

    def forward(self, x):
        # 첫 번째 합성곱 -> ReLU -> 풀링
        x = self.pool(self.relu(self.conv1(x)))
        
        # 두 번째 합성곱 -> ReLU -> 풀링
        x = self.pool(self.relu(self.conv2(x)))
        
        # 세 번째 합성곱 -> ReLU -> 풀링 없이 바로 사용
        x = self.relu(self.conv3(x))

        # 텐서를 펼쳐 완전 연결층에 입력 (크기: [배치 크기, 64 * 3 * 3])
        x = x.view(-1, 64 * 3 * 3)

        # 완전 연결층으로 최종 출력 (크기: [배치 크기, 10])
        x = self.fc(x)
        
        return x

# Data Load 
transform = transforms.ToTensor()
train_data = datasets.MNIST(root="C:/Users/jeonh/dataset/MNIST", train=True, transform=transform, download=True)
test_data = datasets.MNIST(root="C:/Users/jeonh/dataset/MNIST", train=False, transform=transform, download=True)

# DataLoader(데이터를 배치 단위로 나눔)
# batch_size=64: 한 번에 64개의 데이터를 불러옴, shuffle=True: 순서 무작위
train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False)

# Initialize neural network(모델 초기화)
model = CNN_han().to(device)

# 손실 함수와 옵티마이저 정의
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01) # SGD사용, 학습률 0.01로 설정

# train_han(훈련 함수) 정의
def train_han(model, loader, optimizer, criterion):
    model.train()  # 훈련 모드 설정
    total_loss = 0   
    correct = 0  
    total = 0  
    loss_arr = []  # 일정 간격마다 배치의 손실을 출력 및 저장함
    
    for i, (images, labels) in enumerate(loader):  # 인덱스와 함께 배치 데이터 가져옴
        images, labels = images.to(device), labels.to(device)  # 데이터를 GPU로 이동

        outputs = model(images)  # 모델을 통해 예측값 계산
        loss = criterion(outputs, labels)  # 손실 함수로 손실 계산

        optimizer.zero_grad()  # 이전 기울기 초기화
        loss.backward()  # 역전파로 기울기 계산
        optimizer.step()  # 옵티마이저로 가중치 업데이트

        total_loss += loss.item()  # 손실 값 누적
        _, predicted = torch.max(outputs, 1)  # 예측값 중 가장 높은 클래스 선택
        correct += (predicted == labels).sum().item()  # 맞춘 데이터 수
        total += labels.size(0)  # 전체 데이터 수 

    avg_loss = total_loss / len(loader)  # 평균 손실 계산
    accuracy = 100 * correct / total  # 정확도 계산(맞춘수/전체의 백분율)
    return avg_loss, accuracy, correct, total, loss_arr  # 최종적으로 평균 손실, 정확도, 맞춘 데이터 수, 전체 데이터 수, 손실 리스트를 반환

# 테스트 함수 정의 
def test_han(model, loader, criterion):
    model.eval()  # 모델을 평가 모드로
    total_loss = 0  
    correct = 0  
    total = 0  

    with torch.no_grad():  # 평가 과정에선 기울기 계산을 비활성화
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)  # GPU 사용
            outputs = model(images)  # 모델을 통해 예측값 계산
            loss = criterion(outputs, labels)  # loss 계산
            total_loss += loss.item()  # loss 값 누적
            _, predicted = torch.max(outputs, 1)  # 예측값 중 가장 높은 클래스 선택
            correct += (predicted == labels).sum().item()  # 맞춘 데이터 수 
            total += labels.size(0)  # 전체 데이터 수 

    avg_loss = total_loss / len(loader)  # 평균 손실 계산
    accuracy = 100 * correct / total  # 정확도 계산(맞춘수/전체의 백분율)
    return avg_loss, accuracy, correct, total  # 최종적으로 평균 손실, 정확도, 맞춘 수, 전체 수 반환

# 학습 루프
num_epochs = 10  # epoch는 10
for epoch in range(num_epochs):
    # 훈련 데이터로 모델 학습
    train_loss, train_accuracy, correct_train, total_train, loss_arr = train_han(model, train_loader, optimizer, criterion)
    # 테스트 데이터로 모델 평가
    test_loss, test_accuracy, correct_test, total_test = test_han(model, test_loader, criterion)

    # 에포크별 훈련 및 테스트 결과 출력
    print(f"Epoch [{epoch+1}/{num_epochs}]")
    print(f"Train Loss: {train_loss:.5f}, Train Accuracy: {train_accuracy:.3f}%")  # 손실값 소수점 다섯째, 정확도 소수점 셋째
    print(f"Test Loss: {test_loss:.5f}, Test Accuracy: {test_accuracy:.3f}%")      # 손실값 소수점 다섯째, 정확도 소수점 셋째
    print(f"Correct Train: {correct_train}/{total_train}, Correct Test: {correct_test}/{total_test}\n")

# 학번과 이름 출력
print('20224309 컴퓨터공학과 전한나')
